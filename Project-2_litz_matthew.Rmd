---
title: "Disaster Relief Project: Part 2"
author: "Matthew Litz"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
  pdf_document: 
    toc: yes
    latex_engine: xelatex
# You can make the format personal - this will get you started:  
# https://bookdown.org/yihui/rmarkdown/html-document.html#appearance_and_style    
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,      # show R warnings
                      message=FALSE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**DS 6030 | Summer 2021 | University of Virginia **

*******************************************

# Introduction 

In 2010, residents of Haiti were displaced from their homes by a devastating earthquake.  Survivors were known to have created makeshift tents using Blue Tarps.  A team from Rochester Institute of Technology was able to collect high-altitude images of the regions of Haiti in which the shelters were captured.  These images could be used to aid rescue workers in identifying the location of the survivors.  However, due to the massive quantity of images taken, an algorithm is needed to quickly identify the Blue Tarp tents.

This analysis seeks to use the RGB data generated from the images and apply classification methods to identify the Blue Tarps and by extension, the survivors.  A reference data set has been provided from which to train the classification algorithms on.  Five different classification algorithms will be applied on the dataset and one will be recommended for use if a similar natural disaster were to strike in the future.

# Training Data / EDA
The initial exploratory data analysis consisted of generating boxplots of the reference dataset objects (Blue Tarp, Rooftop, Soil, Various Non-Tarp, Vegetation) and the associated RGB colors.  The boxplot for the color "blue" shows a clear higher median for identifying Blue Tarps, therefore producing confidence that of the 3 RGB predictors used in the classification, we appear to have at least one strong response.  In addition, the correlation plot generated from the GGally package suggests that the color green could also have a strong predictive response.

Since we are solely interested in the Blue Tarp class, the classification models will be generated by predicting whether or not a given data set is a Blue Tarp.  To achieve this, a new column was created which specifies if the image is a Blue Tarp.


#NOTES to confirm, we pick the threshold based on the TRAINING Data? And then apply to the hold out data, correct? 

External holdout is just there to estimate the error and various metrics

do two models with cross validation

how did we pick threshhold



exclude data with high values of red?

PCA might me interesting


Summary from last week:

Caret
        “Learned caret too late”
        helpful due to CV capabilities
        consistent interface for all methods
        easy to get metrics
Code execution
        Running code in RStudio works, but knitting fails. This is due to results being left over from changed code (Clear your environment from time to time and rerun everything in RStudio)
        Use caching to avoid recalculation (be careful if code chunks depend on previous results; consider caching only code chunks that take a long time)
        Caret: with large number of tuning parameters, consider reducing the number of explored combinations using tunelength
        caret: set allowParallel=TRUE
Class imbalance
        Reduce amount of non-tarp
        FPR is pointless to look at
Feature engineering
        Interaction blue and green
        Convert RGB to HSV colors
        Location data would be a useful additional feature
EDA learning points
        blue well separated; expected KNN to work better
        collinearity and brightness
        duplicating blue tarp adds little value (problematic for KNN!)
Threshold selection
        improve model performance
Penalized ridge logistic regression
        lambda very small, therefore no need for ridge or lasso



Notes from my grade

EDA
        you can also try scatter plot to see the cluster of data
    Model training
        set-up
            Don't hide the code
        You need to explain what you are doing, how you do this, rather than just put the code here
        KNN
            why only 5, 7, 9? did you try other values? 
        threshold
            what about the threshold of logistic regression, LDA, QDA?
        ROC curves
            the ROC curves seems not very correct. the accuracy of your model are very high, the ROC curve should not be poor like this
        Conclusion
            need at least 3 conclusions.




```{r load-packages}
# Load Required Packages
library(tidyverse)
library(readr)
library(GGally)
library(gridExtra)

data = read_csv('HaitiPixels.csv')
attach(data)

data = data %>% 
  mutate(BlueTarp = if_else(Class == "Blue Tarp", "yes", "no"))






par(mfrow=c(3,1))
bp_blue = ggplot(data, aes(x=factor(Class), y=Blue)) + 
  geom_boxplot(fill="blue", color="black") + 
  labs(x="Class", y="Blue")

bp_red = ggplot(data, aes(x=factor(Class), y=Red)) + 
  geom_boxplot(fill="red", color="black") + 
  labs(x="Class", y="Red")

bp_gr = ggplot(data, aes(x=factor(Class), y=Green)) + 
  geom_boxplot(fill="green", color="black") + 
  labs(x="Class", y="Green")


require(gridExtra)
grid.arrange(bp_blue, bp_red, bp_gr, ncol=2)




#remove classification variables
df=subset(data, select = -c(Class))

```

## PAreto and PCA

The reduced dataset allows us tp
PCA provides a tool to do just this. It finds a low-dimensional representation of a data set that contains as much as possible of the variation.

```{r}
pareto_df <- data %>%
  count(Class, sort = TRUE) 
pareto_df


ggplot(pareto_df, aes(x=reorder(Class,-n), y=n)) + 
  geom_bar(stat = "identity") + 
  labs(x="Class", y="Count")


class_df = subset(data, select = -c(BlueTarp))
num_df = subset(data, select = -c(BlueTarp,Class))

#pca
pr.out = prcomp(num_df, scale=TRUE)
biplot(pr.out, scale =0)
summary(pr.out)
plot(pr.out)



df_pca = data.frame(
  PCA1 = pr.out$x[, 1],
  PCA2 = pr.out$x[, 2],
  label = data$Class,
  colour = Cols(data$Class)
)

ggplot(df_pca, aes(x=PCA1, y=PCA2, color=label)) +
  geom_point()




```




# Data Reduction

As the pareto charts shows, there is an imbalance of Blue Tarp data in the data set.  Therefore we must examine the distributions of RGB in the other classifications.

Let's examine the top two datasets 


Will it affect blue classification?

need to split to maintain distributions

add facet

```{r}
library(reshape2)
#need to mutate dataframe
vegetation_df = data[data$Class=="Vegetation",]
vegetation_df=subset(vegetation_df, select = -c(Class,BlueTarp))
vegetation_df=as_tibble(melt(vegetation_df))

soil_df = data[data$Class=="Soil",]
soil_df=subset(soil_df, select = -c(Class,BlueTarp))
soil_df=as_tibble(melt(soil_df))

rooftop_df = data[data$Class=="Rooftop",]
rooftop_df=subset(rooftop_df, select = -c(Class,BlueTarp))
rooftop_df=as_tibble(melt(rooftop_df))

bluetarp_df = data[data$Class=="Blue Tarp",]
bluetarp_df=subset(bluetarp_df, select = -c(Class,BlueTarp))
bluetarp_df=as_tibble(melt(bluetarp_df))


par(mfrow=c(2,2))
veg_dist <- ggplot(data=vegetation_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Vegetation")



soil_dist = ggplot(data=soil_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Soil")


rooftop_dist = ggplot(data=rooftop_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Rooftop")


bluetarp_dist = ggplot(data=bluetarp_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Blue Tarp")



grid.arrange(veg_dist, soil_dist, rooftop_dist,bluetarp_dist, ncol=2)

```





Next we will sample smaller datasizes from Soil and Vegetation

```{r}


vegsub.rows = sample(rownames(vegetation_df), dim(vegetation_df)[1]*0.3)
vegsub = vegetation_df[vegsub.rows, ]


vegsub_dist <- ggplot(data=vegsub, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Vegetation subset")




soilsub.rows = sample(rownames(soil_df), dim(soil_df)[1]*0.3)
soilsub = soil_df[vegsub.rows, ]


soilsub_dist <- ggplot(data=soilsub, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Soil subset")



grid.arrange(vegsub_dist, soilsub_dist, ncol=2)





```



## Create reduced dataframe
The data is imbalanced
```{r}

#subset dataframes and then merge into new dataframe for testing
vegetation_df = data[data$Class=="Vegetation",]
vegsub.rows = sample(rownames(vegetation_df), dim(vegetation_df)[1]*0.1)
vegsub = vegetation_df[vegsub.rows, ]

soil_df = data[data$Class=="Soil",]
soilsub.rows = sample(rownames(soil_df), dim(soil_df)[1]*0.1)
soilsub = soil_df[soilsub.rows, ]


rooftop_df = data[data$Class=="Rooftop",]
roof_sub.rows = sample(rownames(rooftop_df), dim(rooftop_df)[1]*0.25)
roof_sub = rooftop_df[roof_sub.rows, ]

nontarp_df = data[data$Class=="Various Non-Tarp",]
nontarp_sub.rows = sample(rownames(nontarp_df), dim(nontarp_df)[1]*0.5)
nontarp_sub = nontarp_df[nontarp_sub.rows, ]



#other dataframes
#nontarp_sub = data[data$Class=="Various Non-Tarp",]
bluetarp_sub = data[data$Class=="Blue Tarp",]


df = bind_rows(vegsub, soilsub, roof_sub, nontarp_sub, bluetarp_sub,id = NULL) 


sub_pareto_df <- df %>%
  count(Class, sort = TRUE) 
sub_pareto_df


ggplot(sub_pareto_df, aes(x=reorder(Class,-n), y=n)) + 
  geom_bar(stat = "identity") + 
  labs(x="Class", y="Count")



df=subset(df, select = -c(Class))





```




# Model Training

## Set-up
The training and testing data is first set up using a 60/40 split.  The caret package is used extensively in this analysis.  10-fold cross validation was used for all models.  The confusion matrix was generated using the "Blue Tarp" factor column in the test dataset.

```{r setup, echo=FALSE, message=FALSE}
library(broom)
library(tidymodels)
library(randomForest)
library(klaR)
library(caret)
library(glmnet)
library(class)
library(pROC)
library(purrr)
library(yardstick)
library(ROCR)
library(randomForest)
library(xgboost)
library(Matrix)
library(magrittr)
library(dplyr)
library(glmnet)
library(e1071)
library(kernlab)

#Create training and test splits
train.rows = sample(rownames(df), dim(df)[1]*0.6)
train = df[train.rows, ]



#train$BlueTarp <- factor(train$BlueTarp, levels = c("yes", "no"))
#levels(train$BlueTarp)
train$BlueTarp = as.factor(train$BlueTarp)
#levels(train$BlueTarp)=c(0,1)
write.csv(train,"train.csv", row.names = FALSE)

test = df[setdiff(rownames(df), train.rows), ]
c(dim(train)[1], dim(test)[1])
test$BlueTarp = as.factor(test$BlueTarp)

#formula used throughout
fmla = as.formula(BlueTarp~.)

#define X.test
X.test = hardhat::mold(fmla, data=test)$predictors

#define factors for Y.test
cm_truth = as.factor(test$BlueTarp)
#levels(cm_truth) = c('yes','no')
write.csv(cm_truth,"cm_truth.csv", row.names = FALSE)
```




## Build Hold-out Datasets
NOTE: The file "orthovnir067_ROI_Blue_Tarps_data.txt" was omitted from the hold out set since it was a copy of the  

```{r hold_out}

non_bluetarp057 = read_delim("orthovnir057_ROI_NON_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
non_bluetarp057 = non_bluetarp057 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "no") %>%
  rename(R = Red) #%>%

bluetarp067 = read_delim("orthovnir067_ROI_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
bluetarp067 = bluetarp067 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "yes") 

non_bluetarp067 = read_delim("orthovnir067_ROI_NOT_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
non_bluetarp067 = non_bluetarp067 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "no") 

bluetarp069 = read_delim("orthovnir069_ROI_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
bluetarp069 = bluetarp069 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "yes") 

non_bluetarp069 = read_delim("orthovnir069_ROI_NOT_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
non_bluetarp069 = non_bluetarp069 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "no") 

bluetarp078 = read_delim("orthovnir078_ROI_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
bluetarp078 = bluetarp078 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "yes") 

non_bluetarp078 = read_delim("orthovnir078_ROI_NON_Blue_Tarps.txt", delim=" ", skip=8,col_names=c("ID","X","Y","MapX","MapY","Lat","Lon","Red","Green","Blue"))
non_bluetarp078 = non_bluetarp078 %>%
  dplyr::select(Red,Green,Blue) %>%
  add_column(BlueTarp = "no") 

#assemble holdout df
holdout_df = bind_rows(non_bluetarp057, bluetarp067, non_bluetarp067, bluetarp069, non_bluetarp069,bluetarp078,non_bluetarp078,id = NULL)

#convert to numeric
holdout_df$Red = as.numeric(holdout_df$Red) 
holdout_df$Green = as.numeric(holdout_df$Green) 
holdout_df$Blue = as.numeric(holdout_df$Blue) 
holdout_df


#pareto
ho_pareto_df <- bluetarp078 %>%
  count(BlueTarp, sort = TRUE) 
ho_pareto_df


ggplot(ho_pareto_df, aes(x=reorder(BlueTarp,-n), y=n)) + 
  geom_bar(stat = "identity") + 
  labs(x="Class", y="Count")







#look at empty rows heatmap?
```







## Logistic Regression Results


```{r log, echo=TRUE, message=FALSE, warning=FALSE}


trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

logitFit = train(fmla, data=train,
                 method="glm",
                 family='binomial',  
                 trControl=trControl)

logitFit

#predict values and eastablish levels
logit_pred = predict(logitFit, X.test, type="prob")
logit_pred_raw = predict(logitFit, X.test, type="raw")
#levels(logit_pred) = c('yes','no')



confusionMatrix(logit_pred_raw, cm_truth)

table(logit_pred_plain, test$BlueTarp)

logit_roc = roc(response=cm_truth,
            predictor=logit_pred$yes,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE)


```






## Linear Discriminant Analysis (LDA) Results


```{r LDA, echo=TRUE}
set.seed(125)

trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

ldaFit = caret::train(BlueTarp ~ ., data=train, 
                 method='lda', 
                 trControl=trControl)

ldaFit
lda_pred = predict(ldaFit, newdata=X.test, type="prob")
lda_pred_plain = predict(ldaFit, X.test)
lda_pred_raw = predict(ldaFit, newdata=X.test, type="raw")

lda_perf = performance(prediction(lda_pred[2],cm_truth),"tpr","fpr")

confusionMatrix(lda_pred_raw, cm_truth)
table(lda_pred_raw, test$BlueTarp)

lda_roc = roc(response=cm_truth,
            predictor=lda_pred$yes,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE,
            main ="LDA ROC - Test Data")

```






## Quadratic Discriminant Analysis (QDA) Results

```{r qda, echo=TRUE}
set.seed(17)

trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

#tuneGrid = expand.grid(lambda=lambdas, alpha=0)

qdaFit = caret::train(BlueTarp ~ ., data=train, 
                 method='qda', 
                 trControl=trControl)
qdaFit

#predict values and establish levels
qda_pred = predict(qdaFit, X.test, type="prob")
qda_pred_raw = predict(qdaFit, X.test, type="raw")


confusionMatrix(qda_pred_raw, cm_truth)

qda_roc = roc(response=cm_truth,
            predictor=qda_pred$yes,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE,
            main ="QDA ROC - Test Data")

#Next we will examine if we adjust the threshold if we improve our accuracy
#threshold analysis


qda_pred_01 = ifelse(qda_pred[2] > 0.1, 1, 0)
table(qda_pred_01, cm_truth)


#threshold analysis
qda_pred_02 = ifelse(qda_pred[2] > 0.2, 1, 0)
table(qda_pred_02, cm_truth)


#threshold analysis
qda_pred_09 = ifelse(qda_pred[2] > 0.9, 1, 0)
table(qda_pred_09, cm_truth)


```


## K-Nearest Neighbors (KNN) results
The ideal tuning parameter for kNN was automatically calculated through the caret package as k=7.

The parameters available for tuning


```{r knn, echo=TRUE}


#ctrl <- trainControl(method="repeatedcv",repeats = 3) #,classProbs=TRUE,summaryFunction = twoClassSummary)
#knnFit <- train(Direction ~ ., data = training, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)

trControl = caret::trainControl(method="repeatedcv", number=3,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

knnFit = caret::train(BlueTarp ~ ., data=train, 
                 method='knn',
                 preProcess = c("center", "scale"),
                 trControl=trControl,
                 tuneLength = 40)

knnFit

#predict values and establish levels
knn_pred = predict(knnFit, X.test, type="prob")
knn_pred_raw = predict(knnFit, X.test, type="raw")
#levels(knn_pred) = c('yes','no')

confusionMatrix(knn_pred_raw, cm_truth)

knn_roc = roc(response=cm_truth,
            predictor=knn_pred$yes,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE,
            main ="KNN ROC - Test Data")

plot(knnFit)
```




```{r}
modelLookup("lda")
```





## Penalized Logistic Regression (PLR)

The final model applied was PLR.  This model utilizes the glmnet model with ridge regression.  The turning parameter that provides the best performance was 0.003981072.	

```{r plr elasticnet, echo=TRUE}

lambdas = 10^seq(0, -4, by=-0.1)

trControl = caret::trainControl(method='cv',
                                number=10,
                                savePredictions=TRUE, 
                                classProbs=TRUE,     
                                allowParallel=TRUE)


plrFit = caret::train(fmla, data=train,
                 method='glmnet',
                 family='binomial', 
                 thresh=0.74,       
                 trControl=trControl,
                 tuneGrid=expand.grid(lambda=lambdas, alpha=0))
plrFit

#best tuning parameter
plrFit$bestTune

#predict values and establish levels
plr_pred = predict(plrFit, X.test, type="prob")
plr_pred_raw = predict(plrFit, X.test, type="raw")

confusionMatrix(plr_pred_raw, cm_truth)

plr_roc = roc(response=cm_truth,
            predictor=plr_pred$yes,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE,
            main ="PLR ROC - Test Data")



plot(plrFit)

```


## Threshold Selection for Penalized Logistic Regression
The threshold that produced the highest accuracy (0.74) was back-inserted into the PLR model.

```{r threshold, echo=FALSE, warning=FALSE}

#Given a tuned model, you can explore different threshold values using caret::thresholder

probs = seq(.1, 0.9, by = 0.02)

ths = thresholder(plrFit,
                   threshold = probs,
                   final = TRUE,
                   statistics = "all")
plot(ths$prob_threshold, ths$Accuracy)

```




### Random Forests 

mtry (only 2 values) and ntrees pa

```{r random_forests, echo=TRUE}

set.seed(1)
#rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
modellist <- list()
trControl=caret::trainControl("cv", 
                              number=10,
                              savePredictions=TRUE,
                              returnResamp='all')
for (ntree in c(100,200,500,1000,2000)){
rf_fit = caret::train(BlueTarp~., data=train, 
                         method='rf', 
                         ntree=ntree,
                         importance=TRUE, 
                         tuneGrid=expand.grid(mtry = 1:3),
                         trControl=trControl)
key = toString(ntree)
  modellist[[key]] <- rf_fit
}


results = resamples(modellist)
summary(results)



#yhat.rf = predict(rf.boston,newdata=Boston[-train,])
#mean((yhat.rf-boston.test)^2)
#rf.caret

```


```{r feat_imp}

modellist$`100`$bestTune
modellist$`200`$bestTune
modellist$`500`$bestTune
modellist$`1000`$bestTune
modellist$`2000`$bestTune

modellist$`500`$results
modellist$`1000`$results

acc_100=plot(modellist$`100`)
acc_200=plot(modellist$`200`)
acc_500=plot(modellist$`500`)
acc_1000=plot(modellist$`1000`)
acc_2000=plot(modellist$`2000`)

grid.arrange(acc_100,acc_200,acc_500, ncol=2)
grid.arrange(acc_1000,acc_2000, ncol=2)

model1=modellist$`1000`
rf_pred = predict(modellist$`1000`, X.test, type="prob")
rf_pred_raw = predict(modellist$`1000`, X.test, type="raw")

#randomForest::importance(modellist[1]$finalModel)
confusionMatrix(cm_truth,rf_pred_raw)

rf_roc = roc(response=cm_truth,
            predictor=rf_pred$yes,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE,
            main ="Random Forests (ntree=1000) ROC - Test Data")

```




### XGBoost 
The objective "binary:logistic logistic" was utilized for this classification problem.

hyperparameters?

```{r xgboost, echo=TRUE}


#train and test matrices
trainm = sparse.model.matrix(BlueTarp ~.-1,data=train)
train_label = as.numeric(as.factor(train$BlueTarp))-1
train_matrix = xgb.DMatrix(data = as.matrix(trainm),label = train_label)

testm = sparse.model.matrix(BlueTarp ~.-1,data=test)
test_label = as.numeric(as.factor(test$BlueTarp))-1
test_matrix = xgb.DMatrix(data = as.matrix(testm),label = test_label)

#Parameters
nc = length(unique(train_label))
xgb_params = list("objective" = "multi:softprob",
                    "eval_metric" = "mlogloss",
                    "num_class" = 2,
                    "learning_rate" = 0.01, #shrinkage
                    "num_parallel_tree"=1000)

watchlist <- list(train = train_matrix, test = test_matrix)

#xgb model
bst_model <- xgb.train(params = xgb_params,
                       data=train_matrix,
                       nrounds = 10,
                       watchlist = watchlist)






#from docs api
#cv = xgb.cv(params = xgb_params, data = train_matrix, nrounds = 100, nthread = 2, nfold = 10)
#print(cv)
#print(cv, verbose=TRUE)


```

#XGBoost parameter tuning
find best number of rounds (nrounds)

```{r xgbcv}
#train and test matrices
trainm = sparse.model.matrix(BlueTarp ~.-1,data=train)
train_label = as.numeric(as.factor(train$BlueTarp))-1
train_matrix = xgb.DMatrix(data = as.matrix(trainm),label = train_label)

testm = sparse.model.matrix(BlueTarp ~.-1,data=test)
test_label = as.numeric(as.factor(test$BlueTarp))-1
test_matrix = xgb.DMatrix(data = as.matrix(testm),label = test_label)

xgb_params = list("objective" = "multi:softprob",
                    "eval_metric" = "auc",
                    "num_class" = 2,
                    "learning_rate" = 0.01, #shrinkage
                    "num_parallel_tree"=1000)

cv = xgb.cv(params = xgb_params, 
            data = train_matrix, 
            nrounds = 10, 
            nthread = 2,
            metrics = "auc",
            nfold = 10,
            early_stopping_rounds = 5,
            maximize = TRUE,
            showsd = TRUE,
            callbacks = list(cb.cv.predict(save_models=TRUE)))
print(cv)
print(cv, verbose=TRUE)







```
```{r xgb.cv.pred}

#length(cv$pred[2])
#length(cm_truth)
#predict values and establish levels
#knn_pred = predict(knnFit, X.test, type="prob")
#knn_pred_raw = predict(knnFit, X.test, type="raw")
#levels(knn_pred) = c('yes','no')

#threshold analysis
xgb_pred_05 = ifelse(cv$pred[,2] > 0.5, 1, 0)
table(xgb_pred_05, train$BlueTarp)

#confusionMatrix(xgb_pred_05, train$BlueTarp)


rocplot(cv$pred[,2], train$BlueTarp, main ="XGBoost ROC - Test Data ")

#knn_perf = performance(prediction(knn_pred[2],cm_truth),"tpr","fpr")
#plot(knnFit)



```




















Visualizing the SHAP feature contribution to prediction dependencies on feature value.

```{r xgb_shap}

xgb.plot.shap(
  data,
  shap_contrib = NULL,
  features = NULL,
  top_n = 1,
  model = NULL,
  trees = NULL,
  target_class = NULL,
  approxcontrib = FALSE,
  subsample = NULL,
  n_col = 1,
  col = rgb(0, 0, 1, 0.2),
  pch = ".",
  discrete_n_uniq = 5,
  discrete_jitter = 0.01,
  ylab = "SHAP",
  plot_NA = TRUE,
  col_NA = rgb(0.7, 0, 1, 0.6),
  pch_NA = ".",
  pos_NA = 1.07,
  plot_loess = TRUE,
  col_loess = 2,
  span_loess = 0.5,
  which = c("1d", "2d"),
  plot = TRUE,
  ...
)

```




### Support Vector Machines (SVM)
Scaling was implemented on this data set
tuneGrid?
use cache

The cost function was tuned in this analysis

Cost and gamma tuned for svm


Note: Are the ROC curves built from in-sample or out-of-sample data?

```{r svm, echo=TRUE}
set.seed(123)

#ranges = list ( cost = c(0.001, 0.01,0.1,1,5,10,100)

radialTune = expand.grid(sigma= 2^c(-25, -20, -15,-10, -5, 0), C= c(0.001, 0.01,0.1,1,5,10,100))
linearTune = expand.grid(C=c(0.001, 0.01,0.1,1,5,10,100))              
                
trControl = caret::trainControl(method="repeatedcv", 
                                repeats=5,
                                savePredictions=TRUE,
                                classProbs = TRUE,
                                allowParallel=TRUE,
                                summaryFunction = twoClassSummary)



svmFit = caret::train(BlueTarp ~ ., data=train, 
                 method='svmRadial', #svmRadial #svmPoly
                 preProcess = c("center", "scale"),
                 metric="ROC",
                 trControl=trControl,
                 tuneGrid = radialTune)

svmFit

#predict values and establish levels
svm_pred = predict(svmFit, X.test, type="prob",drop=FALSE)


#confusionMatrix(knn_pred, cm_truth)

rocplot(svm_pred[2], cm_truth, main ="Training Data ")
svm_perf = performance(prediction(svm_pred[2],cm_truth),"tpr","fpr")

#kernlab::plot(svmFit$finalModel)

```





```{r svm_base r}

set.seed(1)


#cross-validation
tune.out=tune(svm,BlueTarp~.,data=train,kernel="linear",
              ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))

summary(tune.out)
bestmod=tune.out$best.model
summary(bestmod)

#cv results
ypred=predict(bestmod,X.test)
table(predict=ypred, truth=cm_truth)

#rocplot(ypred, cm_truth, main ="Training Data ")
#svm_perf = performance(prediction(ypred,cm_truth),"tpr","fpr")


```

## SVM ROC Curve
To produce the ROC curve, the tuned parameters from the CV must be inseerted to obtain the fitted values.

However, it is also possible to obtain fitted values for each observation,
which are the numerical scores used to obtain the class labels.

However, these ROC curves are all on the training data. We are really
more interested in the level of prediction accuracy on the test data.

```{r svm_roc_linear}


svmfit.opt = svm(BlueTarp~.,data=test, kernel="linear", cost =100, decision.values=T)

fitted = attributes(predict(svmfit.opt,test,decision.values = TRUE))$decision.values*-1

rf_roc = roc(response=cm_truth,
            predictor=fitted,
            plot=TRUE,
            col='red',
            lwd=4,
            print.auc=TRUE,
            main ="SVM (Linear) ROC - Test Data")

plot(tune.out)

```








```{r svm_plot}

plot(tune.out$best.model, train, Blue~Green, 
     slice=list(Red=median(train$Red),
                Blue=median(train$Blue),
                Green=median(train$Green)), color.palette = topo.colors)


plot(tune.out$best.model, train, Blue~Red, 
     slice=list(Red=median(train$Red),
                Blue=median(train$Blue),
                Green=median(train$Green)), color.palette = topo.colors)


```






```{r svm radial-poly}

#radial basis kernel
tune.out_rad=tune(svm,BlueTarp~.,data=train, kernel="radial",
                  ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out_rad)

bestmod_rad=tune.out_rad$best.model
summary(bestmod_rad)

#cv results for radial kernel
ypred_rad=predict(bestmod_rad,X.test)
table(predict=ypred_rad, truth=cm_truth)



#polynomial basis kernel
tune.out_poly=tune(svm,BlueTarp~.,data=train, kernel="polynomial",
                   ranges=list(cost=c(0.1,1,10,100,1000),d=c(1,2,3,4,5,10)))
summary(tune.out_poly)

bestmod_poly=tune.out_poly$best.model
summary(bestmod_poly)

#cv results for polynomial basis kernel
ypred_poly=predict(bestmod_poly,X.test)
table(predict=ypred_poly, truth=cm_truth)

```


```{r svm_plot}

plot(tune.out_rad$best.model, train, Blue~Red, 
     slice=list(Red=median(train$Red),
                Blue=median(train$Blue),
                Green=median(train$Green)), color.palette = topo.colors)

plot(tune.out_rad$best.model, train, Blue~Green, 
     slice=list(Red=median(train$Red),
                Blue=median(train$Blue),
                Green=median(train$Green)), color.palette = topo.colors)


plot(tune.out_poly$best.model, train, Blue~Red, 
     slice=list(Red=median(train$Red),
                Blue=median(train$Blue),
                Green=median(train$Green)), color.palette = topo.colors)

plot(tune.out_poly$best.model, train, Blue~Green, 
     slice=list(Red=median(train$Red),
                Blue=median(train$Blue),
                Green=median(train$Green)), color.palette = topo.colors)


```


The following values were identified as optimal:

radial
Cost = 1000
gamma = 0.5

polynomial
Cost = 1000
Degree= 3


Error plot are presented of the results of both kernels, with the darkest shades representing the lowest error.

```{r svm_roc_radial_poly}

#radial
svmfit.opt = svm(BlueTarp~.,data=test, kernel="radial", cost =100, gamma= 0.5, decision.values=T)

fitted_rad = attributes(predict(svmfit.opt,test,decision.values = TRUE))$decision.values*-1

svm_poly_roc = roc(response=cm_truth,
            predictor=fitted_rad,
            plot=TRUE,
            col='red',
            lwd=1,
            print.auc=TRUE,
            main ="SVM (Radial) ROC - Test Data")

plot(tune.out_rad)

rf_pred_raw = predict(modellist$`1000`, X.test, type="raw")

#randomForest::importance(modellist[1]$finalModel)
confusionMatrix(cm_truth,ypred)


#poly
svmfit.opt = svm(BlueTarp~.,data=test, kernel="polynomial", cost =100, decision.values=T)

fitted_poly = attributes(predict(svmfit.opt,test,decision.values = TRUE))$decision.values*-1

rf_roc = roc(response=cm_truth,
            predictor=fitted_poly,
            plot=TRUE,
            col='red',
            lwd=3,
            print.auc=TRUE,
            main ="SVM (Poly) ROC - Test Data")

plot(tune.out_poly)

```









```{r roc_curve_assembly}



plot(log_perf, legend = TRUE, title='log', col='red')
plot(lda_perf, add=TRUE, legend = TRUE, title='log', col='blue')
plot(qda_perf, add=TRUE, legend = TRUE, title='log', col='green')
plot(knn_perf, add=TRUE, legend = TRUE, title='log', col='violet')
plot(plr_perf, add=TRUE, legend = TRUE, title='log', col='orange')
plot(svm_perf, add=TRUE, legend = TRUE, title='log', col='black')

legend(0.8, 0.6, legend=c("Log", "LDA","QDA","KNN","PLR","SVM"),
       col=c("red", "blue","green","violet","orange","black"), lty=1:6, cex=0.8)

```








# Results (Cross-Validation)
The following table presents a table of key metrics collected during this analysis:


| Model  	| Tuning  	|  AUROC 	|  Threshold 	|  Accuracy 	| TPR  	| FPR  	| Precision	|
|---	    |---       	|---	    |---	        |---	        |---    |---	  |---	      |
|  Log Reg 	| N/A  	| 0.6496648  	|  N/A 	|  0.995 	|  0.9986 	|  0.1165 	|  0.9963 	|  
|  LDA  	|  N/A 	| 0.7714629  	|  N/A  	| 0.985  	| 0.9907  	|0.1933   	| 0.9983  	| 
|  QDA 	| N/A  	| 0.6103066  	| N/A  	| 0.9949  	|  0.9996 	| 0.1498  	|  0.9952 	| 
|  KNN  	| N/A  	| 0.5571301  	|  N/A 	|  0.9977 	| 0.9985  	|0.0282   	| 0.9991  	| 
|  Penalized Log Reg 	| 0.003981072  	| 1.0000000  	|  0.74 	|  0.9757 	| 1  	| 0.7875  	|  0.9755 	|  
|  Random Forests 	| -  	| -  	|  - 	|  - 	| -  	| -  	|  - 	|  
|  XGBoost 	| -  	| - 	|  - 	|  - 	| -  	| -  	|  - 	|  
|  SVM (kernel) 	| -  	| - 	|  - 	|  - 	| -  	| -  	|  - 	|  





| kernel             	| training error rate 	| test error rate 	|
|--------------------	|---------------------	|-----------------	|
| linear             	| 0.29875             	| 0.3407407       	|
| optimal linear     	| 0.16125             	| 0.1666667       	|
| radial             	| 0.16125             	| 0.1666667       	|
| optimal radial     	| 0.12                	| 0.2333333       	|
| polynomial         	| 0.16125             	| 0.1666667       	|
| optimal polynomial 	| 0.15875             	| 0.1925926       	|









# Conclusions
Judging the methods by the AUC, which provides a cumulative measure of performance, PLR is the superior classification method for this application.  Of the 5 classification methods tested, only PLR and LDA are recommended for use in future applications.

Penalized Linear Regression is the recommended method for identifying survivors in Blue Tarps.  This is further substantiated by its AUC value of 1.  This method has shifted potential false negatives to false positives and true negatives when compared to the other methods.  The test data results show the PLR never classifies a Blue Tarp incorrectly, which is critical for search and rescue operations. 