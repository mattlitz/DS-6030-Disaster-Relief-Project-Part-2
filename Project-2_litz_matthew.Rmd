---
title: "Disaster Relief Project: Part I"
author: "Matthew Litz"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
  pdf_document: 
    toc: yes
    latex_engine: xelatex
# You can make the format personal - this will get you started:  
# https://bookdown.org/yihui/rmarkdown/html-document.html#appearance_and_style    
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=TRUE,      # show R warnings
                      message=TRUE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**DS 6030 | Summer 2021 | University of Virginia **

*******************************************

# Introduction 

In 2010, residents of Haiti were displaced from their homes by a devastating earthquake.  Survivors were known to have created makeshift tents using Blue Tarps.  A team from Rochester Institute of Technology was able to collect high-altitude images of the regions of Haiti in which the shelters were captured.  These images could be used to aid rescue workers in identifying the location of the survivors.  However, due to the massive quantity of images taken, an algorithm is needed to quickly identify the Blue Tarp tents.

This analysis seeks to use the RGB data generated from the images and apply classification methods to identify the Blue Tarps and by extension, the survivors.  A reference data set has been provided from which to train the classification algorithms on.  Five different classification algorithms will be applied on the dataset and one will be recommended for use if a similar natural disaster were to strike in the future.

# Training Data / EDA
The initial exploratory data analysis consisted of generating boxplots of the reference dataset objects (Blue Tarp, Rooftop, Soil, Various Non-Tarp, Vegetation) and the associated RGB colors.  The boxplot for the color "blue" shows a clear higher median for identifying Blue Tarps, therefore producing confidence that of the 3 RGB predictors used in the classification, we appear to have at least one strong response.  In addition, the correlation plot generated from the GGally package suggests that the color green could also have a strong predictive response.

Since we are solely interested in the Blue Tarp class, the classification models will be generated by predicting whether or not a given data set is a Blue Tarp.  To achieve this, a new column was created which specifies if the image is a Blue Tarp.


```{r load-packages, warning=FALSE, message=FALSE}
# Load Required Packages
library(tidyverse)
library(readr)
library(GGally)

data <- read_csv('HaitiPixels.csv')
attach(data)

data <- data %>% 
  mutate(BlueTarp = if_else(Class == "Blue Tarp", "yes", "no"))

ggplot(data, aes(x=factor(Class), y=Blue)) + 
  geom_boxplot(fill="blue", color="black") + 
  labs(x="Class", y="Blue")

ggplot(data, aes(x=factor(Class), y=Red)) + 
  geom_boxplot(fill="red", color="black") + 
  labs(x="Class", y="Red")

ggplot(data, aes(x=factor(Class), y=Green)) + 
  geom_boxplot(fill="green", color="black") + 
  labs(x="Class", y="Green")


#remove classification variables
df=subset(data, select = -c(Class))

#data distributions
df %>% ggpairs(upper=list(continuous=wrap("cor", size=3)))

```


# Model Training

## Set-up
The training and testing data is first set up using a 60/40 split.  The caret package is used extensively in this analysis.  10-fold cross validation was used for all models.  The confusion matrix was generated using the "Blue Tarp" factor column in the test dataset.

```{r setup, echo=FALSE, message=FALSE}
library(broom)
library(tidymodels)
library(randomForest)
library(klaR)
library(caret)
library(glmnet)
library(class)
library(pROC)
library(purrr)
library(yardstick)


#Create training and test splits
train.rows <- sample(rownames(df), dim(df)[1]*0.6)
train <- df[train.rows, ]
test <- df[setdiff(rownames(df), train.rows), ]
c(dim(train)[1], dim(test)[1])

#formula used throughout
fmla = as.formula(BlueTarp~.)

#define X.test
X.test = hardhat::mold(fmla, data=test)$predictors

#define factors for Y.test
cm_truth <- as.factor(test$BlueTarp)
levels(cm_truth) <- c('yes','no')

```





## Logistic Regression Results


```{r log, echo=TRUE, message=FALSE, warning=FALSE}


trControl <- caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

logitFit = train(fmla, data=df,
                 method="glm",
                 family='binomial',  
                 trControl=trControl)

logitFit

#predict values and eastablish levels
logit_pred <- predict(logitFit, X.test, type="raw")
levels(logit_pred) <- c('yes','no')

confusionMatrix(logit_pred, cm_truth)

```



## Linear Discriminant Analysis (LDA) Results


```{r LDA, echo=TRUE}
set.seed(125)

trControl <- caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

ldaFit = caret::train(BlueTarp ~ ., data=df, 
                 method='lda', 
                 trControl=trControl)

ldaFit

#predict values and establish levels
lda_pred <- predict(ldaFit, X.test, type="raw")
levels(lda_pred) <- c('yes','no')

confusionMatrix(lda_pred, cm_truth)

```



## Quadratic Discriminant Analysis (QDA) Results

```{r qda, echo=TRUE}
set.seed(17)

trControl <- caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

qdaFit = caret::train(BlueTarp ~ ., data=df, 
                 method='qda', 
                 trControl=trControl)
qdaFit

#predict values and establish levels
qda_pred <- predict(qdaFit, X.test, type="raw")
levels(qda_pred) <- c('yes','no')

confusionMatrix(qda_pred, cm_truth)



```


## K-Nearest Neighbors (KNN) results
The ideal tuning parameter for kNN was automatically calculated through the caret package as k=7.

```{r knn, echo=TRUE}

trControl <- caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

knnFit = caret::train(BlueTarp ~ ., data=df, 
                 method='knn',
                 preProcess = c("center", "scale"),
                 trControl=trControl)

knnFit

#predict values and establish levels
knn_pred <- predict(knnFit, X.test, type="raw")
levels(knn_pred) <- c('yes','no')

confusionMatrix(knn_pred, cm_truth)


```


## Penalized Logistic Regression (PLR)

The final model applied was PLR.  This model utilizes the glmnet model with ridge regression.  The turning parameter that provides the best performance was 0.003981072.	

```{r plr elasticnet, echo=TRUE}

lambdas = 10^seq(0, -4, by=-0.1)

trControl = caret::trainControl(method='cv',
                                number=10,
                                savePredictions=TRUE, 
                                classProbs=TRUE,     
                                allowParallel=TRUE)

tuneGrid = expand.grid(lambda=lambdas, alpha=0)

plrFit = caret::train(fmla, data=df,
                 method='glmnet',
                 family='binomial', 
                 thresh=0.74,       
                 trControl=trControl,
                 tuneGrid=tuneGrid)
plrFit

#best tuning parameter
plrFit$bestTune

#predict values and establish levels
plr_pred <- predict(plrFit, X.test, type="raw")
levels(plr_pred) <- c('yes','no')

confusionMatrix(plr_pred, cm_truth)

```


## Threshold Selection for Penalized Logistic Regression
The threshold that produced the highest accuracy (0.74) was back-inserted into the PLR model.

```{r threshold, echo=FALSE, warning=FALSE}

#Given a tuned model, you can explore different threshold values using caret::thresholder

probs <- seq(.1, 0.9, by = 0.02)

ths <- thresholder(plrFit,
                   threshold = probs,
                   final = TRUE,
                   statistics = "all")
plot(ths$prob_threshold, ths$Accuracy)

```


### ROC Curves
The probabilities were calculated and assembled into a tibble for all 5 models.

```{r ROC_data, echo=TRUE}


#calculate probabilities
preds = tibble(
  logistic = predict(logitFit, X.test, type="prob")[,1],
  LDA = predict(ldaFit, newdata=X.test, type='prob')[,1],
  QDA = predict(qdaFit, newdata=X.test, type='prob')[,1],
  KNN = predict(knnFit, newdata=X.test, type='prob')[,1],
  PLR = predict(plrFit, newdata=X.test, type='prob')[,1]
)

#Y data from test data
Y.test <- test$BlueTarp

#Calculate evaluation data by which to determine ROC Curves
eval_data = preds %>% 
  mutate(truth = Y.test) %>% 
  pivot_longer(cols=-truth, names_to="model", values_to="score")  %>% 
  rowwise() %>%
  mutate(prob = if_else(truth == "yes", 1-score, score))

```



Using the probability data, the ROC curves were calculated using the yardstick package.

```{r roc_curves}

outcome_levels = c("no", "yes") 
eval_data = eval_data %>% 
  mutate(truth = factor(truth, outcome_levels))
  

#-- Make ROC curve data
ROC_data = eval_data %>% 
  group_by(model) %>% 
  yardstick::roc_curve(truth,prob) 

#PlotROC curves
ROC_data %>% ggplot2::autoplot()   

#calculate AUC
auc = eval_data %>% 
  group_by(model) %>%
  yardstick::roc_auc(truth, prob) 
auc

```

# Results (Cross-Validation)
The following table presents a table of key metrics collected during this analysis:


| Model  	| Tuning  	|  AUROC 	|  Threshold 	|  Accuracy 	| TPR  	| FPR  	| Precision	|
|---	|---	|---	|---	|---	|---	|---	|---	|
|  Log Reg 	| N/A  	| 0.6496648  	|  N/A 	|  0.995 	|  0.9986 	|  0.1165 	|  0.9963 	|  
|  LDA  	|  N/A 	| 0.7714629  	|  N/A  	| 0.985  	| 0.9907  	|0.1933   	| 0.9983  	| 
|  QDA 	| N/A  	| 0.6103066  	| N/A  	| 0.9949  	|  0.9996 	| 0.1498  	|  0.9952 	| 
|  KNN  	| N/A  	| 0.5571301  	|  N/A 	|  0.9977 	| 0.9985  	|0.0282   	| 0.9991  	| 
|  Penalized Log Reg 	| 0.003981072  	| 1.0000000  	|  0.74 	|  0.9757 	| 1  	| 0.7875  	|  0.9755 	|  


# Conclusions
Judging the methods by the AUC, which provides a cumulative measure of performance, PLR is the superior classification method for this application.  Of the 5 classification methods tested, only PLR and LDA are recommended for use in future applications.

Penalized Linear Regression is the recommended method for identifying survivors in Blue Tarps.  This is further substantiated by its AUC value of 1.  This method has shifted potential false negatives to false positives and true negatives when compared to the other methods.  The test data results show the PLR never classifies a Blue Tarp incorrectly, which is critical for search and rescue operations. 