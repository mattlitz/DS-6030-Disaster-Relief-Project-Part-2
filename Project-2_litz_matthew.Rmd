---
title: "Disaster Relief Project: Part 2"
author: "Matthew Litz"
date: "`r format(Sys.Date(), '%b %d, %Y')`"
output:
  html_document:
    number_sections: true    
    toc: true
    toc_float: true
    theme: cosmo
    highlight: espresso    
  pdf_document: 
    toc: yes
    latex_engine: xelatex
# You can make the format personal - this will get you started:  
# https://bookdown.org/yihui/rmarkdown/html-document.html#appearance_and_style    
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->

```{r global_options, include=FALSE}
knitr::opts_chunk$set(error=TRUE,        # Keep compiling upon error
                      collapse=FALSE,    # collapse by default
                      echo=TRUE,         # echo code by default
                      comment = "#>",    # change comment character
                      fig.width = 5.5,     # set figure width
                      fig.align = "center",# set figure position
                      out.width = "49%", # set width of displayed images
                      warning=FALSE,      # show R warnings
                      message=FALSE)      # show R messages
```

<!--- Change font sizes (or other css modifications) --->
<style>
h1.title {
  font-size: 2.2em; /* Title font size */
}
h1 {
  font-size: 2em;   /* Header 1 font size */
}
h2 {
  font-size: 1.5em;
}
h3 { 
  font-size: 1.2em;
}
pre {
  font-size: 0.8em;  /* Code and R output font size */
}
</style>



**DS 6030 | Summer 2021 | University of Virginia **

*******************************************

# Introduction 

In 2010, residents of Haiti were displaced from their homes by a devastating earthquake.  Survivors were known to have created makeshift tents using Blue Tarps.  A team from Rochester Institute of Technology was able to collect high-altitude images of the regions of Haiti in which the shelters were captured.  These images could be used to aid rescue workers in identifying the location of the survivors.  However, due to the massive quantity of images taken, an algorithm is needed to quickly identify the Blue Tarp tents.

This analysis seeks to use the RGB data generated from the images and apply classification methods to identify the Blue Tarps and by extension, the survivors.  A reference data set has been provided from which to train the classification algorithms on.  Five different classification algorithms will be applied on the dataset and one will be recommended for use if a similar natural disaster were to strike in the future.

# Training Data / EDA
The initial exploratory data analysis consisted of generating boxplots of the reference dataset objects (Blue Tarp, Rooftop, Soil, Various Non-Tarp, Vegetation) and the associated RGB colors.  The boxplot for the color "blue" shows a clear higher median for identifying Blue Tarps, therefore producing confidence that of the 3 RGB predictors used in the classification, we appear to have at least one strong response.  In addition, the correlation plot generated from the GGally package suggests that the color green could also have a strong predictive response.

Since we are solely interested in the Blue Tarp class, the classification models will be generated by predicting whether or not a given data set is a Blue Tarp.  To achieve this, a new column was created which specifies if the image is a Blue Tarp.


#NOTES to confirm, we pick the threshold based on the TRAINING Data? And then apply to the hold out data, correct? 

External holdout is just there to estimate the error and various metrics

do two models with cross validation

how did we pick threshhold



exclude data with high values of red?

PCA might me interesting


Summary from last week:

Caret
        “Learned caret too late”
        helpful due to CV capabilities
        consistent interface for all methods
        easy to get metrics
Code execution
        Running code in RStudio works, but knitting fails. This is due to results being left over from changed code (Clear your environment from time to time and rerun everything in RStudio)
        Use caching to avoid recalculation (be careful if code chunks depend on previous results; consider caching only code chunks that take a long time)
        Caret: with large number of tuning parameters, consider reducing the number of explored combinations using tunelength
        caret: set allowParallel=TRUE
Class imbalance
        Reduce amount of non-tarp
        FPR is pointless to look at
Feature engineering
        Interaction blue and green
        Convert RGB to HSV colors
        Location data would be a useful additional feature
EDA learning points
        blue well separated; expected KNN to work better
        collinearity and brightness
        duplicating blue tarp adds little value (problematic for KNN!)
Threshold selection
        improve model performance
Penalized ridge logistic regression
        lambda very small, therefore no need for ridge or lasso




```{r load-packages}
# Load Required Packages
library(tidyverse)
library(readr)
library(GGally)

data = read_csv('HaitiPixels.csv')
attach(data)

data = data %>% 
  mutate(BlueTarp = if_else(Class == "Blue Tarp", "yes", "no"))







ggplot(data, aes(x=factor(Class), y=Blue)) + 
  geom_boxplot(fill="blue", color="black") + 
  labs(x="Class", y="Blue")

ggplot(data, aes(x=factor(Class), y=Red)) + 
  geom_boxplot(fill="red", color="black") + 
  labs(x="Class", y="Red")

ggplot(data, aes(x=factor(Class), y=Green)) + 
  geom_boxplot(fill="green", color="black") + 
  labs(x="Class", y="Green")


#remove classification variables
df=subset(data, select = -c(Class))

```

```{r}
pareto_df <- data %>%
  count(Class, sort = TRUE) 
pareto_df


ggplot(pareto_df, aes(x=reorder(Class,-n), y=n)) + 
  geom_bar(stat = "identity") + 
  labs(x="Class", y="Count")

```




# Data Reduction

As the pareto charts shows, there is an imbalance of Blue Tarp data in the data set.  Therefore we must examine the distributions of RGB in the other classifications.

Let's examine the top two datasets 


Will it affect blue classification?

need to split to maintain distributions


```{r}
library(reshape2)
#need to mutate dataframe
vegetation_df = data[data$Class=="Vegetation",]
vegetation_df=subset(vegetation_df, select = -c(Class,BlueTarp))
vegetation_df=as_tibble(melt(vegetation_df))

soil_df = data[data$Class=="Soil",]
soil_df=subset(soil_df, select = -c(Class,BlueTarp))
soil_df=as_tibble(melt(soil_df))

rooftop_df = data[data$Class=="Rooftop",]
rooftop_df=subset(rooftop_df, select = -c(Class,BlueTarp))
rooftop_df=as_tibble(melt(rooftop_df))

bluetarp_df = data[data$Class=="Blue Tarp",]
bluetarp_df=subset(bluetarp_df, select = -c(Class,BlueTarp))
bluetarp_df=as_tibble(melt(bluetarp_df))



veg_dist <- ggplot(data=vegetation_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Vegetation")

veg_dist

soil_dist <- ggplot(data=soil_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Soil")
soil_dist

rooftop_dist <- ggplot(data=rooftop_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Rooftop")
rooftop_dist

bluetarp_dist <- ggplot(data=bluetarp_df, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Blue Tarp")
bluetarp_dist

```





Next we will sample smaller datasizes from Soil and Vegetation

```{r}


vegsub.rows = sample(rownames(vegetation_df), dim(vegetation_df)[1]*0.3)
vegsub = vegetation_df[vegsub.rows, ]


vegsub_dist <- ggplot(data=vegsub, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Vegetation subset")

vegsub_dist


soilsub.rows = sample(rownames(soil_df), dim(soil_df)[1]*0.3)
soilsub = soil_df[vegsub.rows, ]


soilsub_dist <- ggplot(data=soilsub, aes(x=value, group=variable, fill=variable)) + geom_density(adjust=1.5, alpha=.4) + 
  ggtitle("Soil subset")

soilsub_dist







```



## Create reduced dataframe

```{r}

#subset dataframes and then merge into new dataframe for testing
vegetation_df = data[data$Class=="Vegetation",]
vegsub.rows = sample(rownames(vegetation_df), dim(vegetation_df)[1]*0.2)
vegsub = vegetation_df[vegsub.rows, ]

soil_df = data[data$Class=="Soil",]
soilsub.rows = sample(rownames(soil_df), dim(soil_df)[1]*0.1)
soilsub = soil_df[soilsub.rows, ]


rooftop_df = data[data$Class=="Rooftop",]
roof_sub.rows = sample(rownames(rooftop_df), dim(rooftop_df)[1]*0.25)
roof_sub = rooftop_df[roof_sub.rows, ]

#other dataframes
nontarp_sub = data[data$Class=="Various Non-Tarp",]
bluetarp_sub = data[data$Class=="Blue Tarp",]


df = bind_rows(vegsub, soilsub, roof_sub, nontarp_sub, bluetarp_sub,id = NULL) 


sub_pareto_df <- df %>%
  count(Class, sort = TRUE) 
sub_pareto_df


ggplot(sub_pareto_df, aes(x=reorder(Class,-n), y=n)) + 
  geom_bar(stat = "identity") + 
  labs(x="Class", y="Count")

df=subset(df, select = -c(Class))

plot(df)

```





# Model Training

## Set-up
The training and testing data is first set up using a 60/40 split.  The caret package is used extensively in this analysis.  10-fold cross validation was used for all models.  The confusion matrix was generated using the "Blue Tarp" factor column in the test dataset.

```{r setup, echo=FALSE, message=FALSE}
library(broom)
library(tidymodels)
library(randomForest)
library(klaR)
library(caret)
library(glmnet)
library(class)
library(pROC)
library(purrr)
library(yardstick)
library(ROCR)
library(MASS)
library(randomForest)
library(xgboost)
library(Matrix)
library(magrittr)
library(dplyr)
library(glmnet)
library(e1071)
library(kernlab)

#Create training and test splits
train.rows = sample(rownames(df), dim(df)[1]*0.6)
train = df[train.rows, ]
test = df[setdiff(rownames(df), train.rows), ]
c(dim(train)[1], dim(test)[1])

#formula used throughout
fmla = as.formula(BlueTarp~.)

#define X.test
X.test = hardhat::mold(fmla, data=test)$predictors

#define factors for Y.test
cm_truth = as.factor(test$BlueTarp)
#levels(cm_truth) = c('yes','no')

```





## Logistic Regression Results


```{r log, echo=TRUE, message=FALSE, warning=FALSE}


trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

logitFit = train(fmla, data=train,
                 method="glm",
                 family='binomial',  
                 trControl=trControl)

logitFit

#predict values and eastablish levels
logit_pred = predict(logitFit, X.test, type="prob")
#levels(logit_pred) = c('yes','no')

#confusionMatrix(logit_pred, cm_truth)
rocplot = function(pred,truth,...){
predob = prediction(pred,truth)
perf = performance(predob,"tpr","fpr")
plot(perf)
}

rocplot(logit_pred[2], cm_truth, main ="Training Data")

log_perf = performance(prediction(logit_pred[2],cm_truth),"tpr","fpr")

```



## Linear Discriminant Analysis (LDA) Results


```{r LDA, echo=TRUE}
set.seed(125)

trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

ldaFit = caret::train(BlueTarp ~ ., data=train, 
                 method='lda', 
                 trControl=trControl)


lda_pred = predict(ldaFit, newdata=X.test, type="prob")
lda_pred

rocplot(lda_pred[2], cm_truth, main ="Training Data ")

lda_perf = performance(prediction(lda_pred[2],cm_truth),"tpr","fpr")

```




















## Quadratic Discriminant Analysis (QDA) Results

```{r qda, echo=TRUE}
set.seed(17)

trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

qdaFit = caret::train(BlueTarp ~ ., data=train, 
                 method='qda', 
                 trControl=trControl)
qdaFit

#predict values and establish levels
qda_pred = predict(qdaFit, X.test, type="prob")
#levels(qda_pred) = c('yes','no')

#confusionMatrix(qda_pred, cm_truth)

rocplot(qda_pred[2], cm_truth, main ="Training Data ")
qda_perf = performance(prediction(qda_pred[2],cm_truth),"tpr","fpr")

```


## K-Nearest Neighbors (KNN) results
The ideal tuning parameter for kNN was automatically calculated through the caret package as k=7.

```{r knn, echo=TRUE}

trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

knnFit = caret::train(BlueTarp ~ ., data=train, 
                 method='knn',
                 preProcess = c("center", "scale"),
                 trControl=trControl)

knnFit

#predict values and establish levels
knn_pred = predict(knnFit, X.test, type="prob")
#levels(knn_pred) = c('yes','no')

#confusionMatrix(knn_pred, cm_truth)

rocplot(knn_pred[2], cm_truth, main ="Training Data ")

knn_perf = performance(prediction(knn_pred[2],cm_truth),"tpr","fpr")

```


## Penalized Logistic Regression (PLR)

The final model applied was PLR.  This model utilizes the glmnet model with ridge regression.  The turning parameter that provides the best performance was 0.003981072.	

```{r plr elasticnet, echo=TRUE}

lambdas = 10^seq(0, -4, by=-0.1)

trControl = caret::trainControl(method='cv',
                                number=10,
                                savePredictions=TRUE, 
                                classProbs=TRUE,     
                                allowParallel=TRUE)

tuneGrid = expand.grid(lambda=lambdas, alpha=0)

plrFit = caret::train(fmla, data=train,
                 method='glmnet',
                 family='binomial', 
                 thresh=0.74,       
                 trControl=trControl,
                 tuneGrid=tuneGrid)
plrFit

#best tuning parameter
plrFit$bestTune

#predict values and establish levels
plr_pred = predict(plrFit, X.test, type="prob")
#levels(plr_pred) = c('yes','no')

#confusionMatrix(plr_pred, cm_truth)

rocplot(plr_pred[2], cm_truth, main ="Training Data ")
plr_perf = performance(prediction(plr_pred[2],cm_truth),"tpr","fpr")

```


## Threshold Selection for Penalized Logistic Regression
The threshold that produced the highest accuracy (0.74) was back-inserted into the PLR model.

```{r threshold, echo=FALSE, warning=FALSE}

#Given a tuned model, you can explore different threshold values using caret::thresholder

probs = seq(.1, 0.9, by = 0.02)

ths = thresholder(plrFit,
                   threshold = probs,
                   final = TRUE,
                   statistics = "all")
plot(ths$prob_threshold, ths$Accuracy)

```



### Random Forests with XGB 
The objective "binary:logistic logistic" was utilized for this classification problem.



```{r random_forests, echo=TRUE}


#train and test matrices
trainm <- sparse.model.matrix(BlueTarp ~.-1,data=train)
train_label <- train$BlueTarp
train_matrix <- xgb.DMatrix(data = as.matrix(trainm),label = train_label)

testm <- sparse.model.matrix(BlueTarp ~.-1,data=test)
test_label <- test$BlueTarp
test_matrix <- xgb.DMatrix(data = as.matrix(testm),label = test_label)

#Parameters
#nc <- length(unique(train_label))
xgb_params <- list("objective" = "binary:logistic",
                    "eval_metric" = "mlogloss",
      #              "num_class" = nc,
                    "learning_rate" = 0.01, #shrinkage
                    "num_parallel_tree"=1000)

#watchlist <- list(train = train_matrix, test = test_matrix)

#xgb model
#bst_model <- xgb.train(params = xgb_params,
#                       data=train_matrix,
#                       nrounds = 100,
#                       watchlist = watchlist)



#cv
#xgb_cv = cv(dtrain=data_dmatrix, params=params, nfold=3,
#                    num_boost_round=50, early_stopping_rounds=10, metrics="auc", as_pandas=True, seed=123)



#from docs api
cv = xgb.cv(params = xgb_params, data = train_matrix, nrounds = 100, nthread = 2, nfold = 10, metrics = list("rmse","auc"),
                  max_depth = 3, eta = 0.1)
print(cv)
print(cv, verbose=TRUE)


```





### Support Vector Machines (SVM)
Scaling was implemented on this data set


```{r svm, echo=TRUE}


trControl = caret::trainControl(method="cv", number=10,
                                     savePredictions=TRUE,
                                     classProbs = TRUE,
                                     allowParallel=TRUE)

svmFit = caret::train(BlueTarp ~ ., data=train, 
                 method='svmLinear',
                 preProcess = c("center", "scale"),
                 trControl=trControl)

svmFit

#predict values and establish levels
svm_pred = predict(svmFit, X.test, type="prob")
#levels(knn_pred) = c('yes','no')

#confusionMatrix(knn_pred, cm_truth)

rocplot(svm_pred[2], cm_truth, main ="Training Data ")
svm_perf = performance(prediction(svm_pred[2],cm_truth),"tpr","fpr")


```



```{r}



plot(log_perf, legend = TRUE, title='log', col='red')
plot(lda_perf, add=TRUE, legend = TRUE, title='log', col='blue')
plot(qda_perf, add=TRUE, legend = TRUE, title='log', col='green')
plot(knn_perf, add=TRUE, legend = TRUE, title='log', col='violet')
plot(plr_perf, add=TRUE, legend = TRUE, title='log', col='orange')
plot(svm_perf, add=TRUE, legend = TRUE, title='log', col='black')

legend(0.8, 0.6, legend=c("Log", "LDA","QDA","KNN","PLR","SVM"),
       col=c("red", "blue","green","violet","orange","black"), lty=1:6, cex=0.8)

```










### Old ROC Curves
The probabilities were calculated and assembled into a tibble for all 5 models.

```{r ROC_data, echo=TRUE}


#calculate probabilities
preds = tibble(
  logistic = predict(logitFit, X.test, type="prob")[,1],
  LDA = predict(ldaFit, newdata=X.test, type='prob')[,1],
  QDA = predict(qdaFit, newdata=X.test, type='prob')[,1],
  KNN = predict(knnFit, newdata=X.test, type='prob')[,1],
  PLR = predict(plrFit, newdata=X.test, type='prob')[,1]
)

#Y data from test data
Y.test = test$BlueTarp

#Calculate evaluation data by which to determine ROC Curves
eval_data = preds %>% 
  mutate(truth = Y.test) %>% 
  pivot_longer(cols=-truth, names_to="model", values_to="score")  %>% 
  rowwise() %>%
  mutate(prob = if_else(truth == "yes", 1-score, score))

```



Using the probability data, the ROC curves were calculated using the yardstick package.

```{r roc_curves}

outcome_levels = c("no", "yes") 
eval_data = eval_data %>% 
  mutate(truth = factor(truth, outcome_levels))
  

#-- Make ROC curve data
ROC_data = eval_data %>% 
  group_by(model) %>% 
  yardstick::roc_curve(truth,prob) 

#PlotROC curves
ROC_data %>% ggplot2::autoplot()   

#calculate AUC
auc = eval_data %>% 
  group_by(model) %>%
  yardstick::roc_auc(truth, prob) 
auc

```

# Results (Cross-Validation)
The following table presents a table of key metrics collected during this analysis:


| Model  	| Tuning  	|  AUROC 	|  Threshold 	|  Accuracy 	| TPR  	| FPR  	| Precision	|
|---	|---	|---	|---	|---	|---	|---	|---	|
|  Log Reg 	| N/A  	| 0.6496648  	|  N/A 	|  0.995 	|  0.9986 	|  0.1165 	|  0.9963 	|  
|  LDA  	|  N/A 	| 0.7714629  	|  N/A  	| 0.985  	| 0.9907  	|0.1933   	| 0.9983  	| 
|  QDA 	| N/A  	| 0.6103066  	| N/A  	| 0.9949  	|  0.9996 	| 0.1498  	|  0.9952 	| 
|  KNN  	| N/A  	| 0.5571301  	|  N/A 	|  0.9977 	| 0.9985  	|0.0282   	| 0.9991  	| 
|  Penalized Log Reg 	| 0.003981072  	| 1.0000000  	|  0.74 	|  0.9757 	| 1  	| 0.7875  	|  0.9755 	|  
|  Random Forests 	| -  	| -  	|  - 	|  - 	| -  	| -  	|  - 	|  
|  SVM 	| -  	| - 	|  - 	|  - 	| -  	| -  	|  - 	|  


# Conclusions
Judging the methods by the AUC, which provides a cumulative measure of performance, PLR is the superior classification method for this application.  Of the 5 classification methods tested, only PLR and LDA are recommended for use in future applications.

Penalized Linear Regression is the recommended method for identifying survivors in Blue Tarps.  This is further substantiated by its AUC value of 1.  This method has shifted potential false negatives to false positives and true negatives when compared to the other methods.  The test data results show the PLR never classifies a Blue Tarp incorrectly, which is critical for search and rescue operations. 